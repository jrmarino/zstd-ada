From d0f59917b31c9ce70fc3a52aa387163871f88f4b Mon Sep 17 00:00:00 2001
From: Matthew Dillon <dillon@apollo.backplane.com>
Date: Sun, 24 Jul 2016 21:49:57 -0700
Subject: [PATCH 060/100] kernel - Add some more options for pmap_qremove*()

* Add pmap_qremove_quick() and pmap_qremove_noinval(), allowing pmap
  entries to be removed without invalidation under carefully managed
  circumstances by other subsystems.

* Redo the virtual kernel a little to work the same as the real kernel
  when entering new pmap entries.  We cannot assume that no invalidation
  is needed when the prior contents of the pte is 0, because there are
  several ways it could have become 0 without a prior invalidation.

  Also use an atomic op to clear the entry.
---
 sys/platform/pc64/x86_64/pmap.c        | 76 ++++++++++++++++++++++++++++------
 sys/platform/vkernel64/platform/pmap.c | 43 ++++++++++++++++---
 sys/vm/pmap.h                          |  2 +
 3 files changed, 103 insertions(+), 18 deletions(-)

diff --git a/sys/platform/pc64/x86_64/pmap.c b/sys/platform/pc64/x86_64/pmap.c
index 7eb2099..998ef4a 100644
--- a/sys/platform/pc64/x86_64/pmap.c
+++ b/sys/platform/pc64/x86_64/pmap.c
@@ -1554,12 +1554,15 @@ pmap_invalidate_range(pmap_t pmap, vm_offset_t sva, vm_offset_t eva)
 }
 
 /*
- * Add a list of wired pages to the kva
- * this routine is only used for temporary
- * kernel mappings that do not need to have
- * page modification or references recorded.
- * Note that old mappings are simply written
- * over.  The page *must* be wired.
+ * Add a list of wired pages to the kva.  This routine is used for temporary
+ * kernel mappings such as those found in buffer cache buffer.  Page
+ * modifications and accesses are not tracked or recorded.
+ *
+ * NOTE! Old mappings are simply overwritten, and we cannot assume relaxed
+ *	 semantics as previous mappings may have been zerod without any
+ *	 invalidation.
+ *
+ * The page *must* be wired.
  */
 void
 pmap_qenter(vm_offset_t beg_va, vm_page_t *m, int count)
@@ -1584,27 +1587,74 @@ pmap_qenter(vm_offset_t beg_va, vm_page_t *m, int count)
 }
 
 /*
- * This routine jerks page mappings from the
- * kernel -- it is meant only for temporary mappings.
+ * This routine jerks page mappings from the kernel -- it is meant only
+ * for temporary mappings such as those found in buffer cache buffers.
+ * No recording modified or access status occurs.
  *
  * MPSAFE, INTERRUPT SAFE (cluster callback)
  */
 void
-pmap_qremove(vm_offset_t va, int count)
+pmap_qremove(vm_offset_t beg_va, int count)
 {
 	vm_offset_t end_va;
+	vm_offset_t va;
 
-	end_va = va + count * PAGE_SIZE;
+	end_va = beg_va + count * PAGE_SIZE;
 
-	while (va < end_va) {
+	for (va = beg_va; va < end_va; va += PAGE_SIZE) {
 		pt_entry_t *pte;
 
 		pte = vtopte(va);
 		(void)pte_load_clear(pte);
 		cpu_invlpg((void *)va);
-		va += PAGE_SIZE;
 	}
-	smp_invltlb();
+	pmap_invalidate_range(&kernel_pmap, beg_va, end_va);
+}
+
+/*
+ * This routine removes temporary kernel mappings, only invalidating them
+ * on the current cpu.  It should only be used under carefully controlled
+ * conditions.
+ */
+void
+pmap_qremove_quick(vm_offset_t beg_va, int count)
+{
+	vm_offset_t end_va;
+	vm_offset_t va;
+
+	end_va = beg_va + count * PAGE_SIZE;
+
+	for (va = beg_va; va < end_va; va += PAGE_SIZE) {
+		pt_entry_t *pte;
+
+		pte = vtopte(va);
+		(void)pte_load_clear(pte);
+		cpu_invlpg((void *)va);
+	}
+}
+
+/*
+ * This routine removes temporary kernel mappings *without* invalidating
+ * the TLB.  It can only be used on permanent kva reservations such as those
+ * found in buffer cache buffers, under carefully controlled circumstances.
+ *
+ * NOTE: Repopulating these KVAs requires unconditional invalidation.
+ *	 (pmap_qenter() does unconditional invalidation).
+ */
+void
+pmap_qremove_noinval(vm_offset_t beg_va, int count)
+{
+	vm_offset_t end_va;
+	vm_offset_t va;
+
+	end_va = beg_va + count * PAGE_SIZE;
+
+	for (va = beg_va; va < end_va; va += PAGE_SIZE) {
+		pt_entry_t *pte;
+
+		pte = vtopte(va);
+		(void)pte_load_clear(pte);
+	}
 }
 
 /*
diff --git a/sys/platform/vkernel64/platform/pmap.c b/sys/platform/vkernel64/platform/pmap.c
index 135f36f..b2a1534 100644
--- a/sys/platform/vkernel64/platform/pmap.c
+++ b/sys/platform/vkernel64/platform/pmap.c
@@ -947,9 +947,8 @@ pmap_qenter(vm_offset_t va, vm_page_t *m, int count)
 		pt_entry_t *pte;
 
 		pte = vtopte(va);
-		if (*pte & VPTE_V)
-			pmap_inval_pte(pte, &kernel_pmap, va);
 		*pte = VM_PAGE_TO_PHYS(*m) | VPTE_RW | VPTE_V | VPTE_U;
+		pmap_inval_pte(pte, &kernel_pmap, va);
 		va += PAGE_SIZE;
 		m++;
 	}
@@ -970,9 +969,43 @@ pmap_qremove(vm_offset_t va, int count)
 		pt_entry_t *pte;
 
 		pte = vtopte(va);
-		if (*pte & VPTE_V)
-			pmap_inval_pte(pte, &kernel_pmap, va);
-		*pte = 0;
+		atomic_swap_long(pte, 0);
+		pmap_inval_pte(pte, &kernel_pmap, va);
+		va += PAGE_SIZE;
+	}
+}
+
+void
+pmap_qremove_quick(vm_offset_t va, int count)
+{
+	vm_offset_t end_va;
+
+	end_va = va + count * PAGE_SIZE;
+	KKASSERT(va >= KvaStart && end_va < KvaEnd);
+
+	while (va < end_va) {
+		pt_entry_t *pte;
+
+		pte = vtopte(va);
+		atomic_swap_long(pte, 0);
+		cpu_invlpg((void *)va);
+		va += PAGE_SIZE;
+	}
+}
+
+void
+pmap_qremove_noinval(vm_offset_t va, int count)
+{
+	vm_offset_t end_va;
+
+	end_va = va + count * PAGE_SIZE;
+	KKASSERT(va >= KvaStart && end_va < KvaEnd);
+
+	while (va < end_va) {
+		pt_entry_t *pte;
+
+		pte = vtopte(va);
+		atomic_swap_long(pte, 0);
 		va += PAGE_SIZE;
 	}
 }
diff --git a/sys/vm/pmap.h b/sys/vm/pmap.h
index a05a388..fe4f4bc 100644
--- a/sys/vm/pmap.h
+++ b/sys/vm/pmap.h
@@ -172,6 +172,8 @@ void		 pmap_pinit2 (pmap_t);
 void		 pmap_protect (pmap_t, vm_offset_t, vm_offset_t, vm_prot_t);
 void		 pmap_qenter (vm_offset_t, struct vm_page **, int);
 void		 pmap_qremove (vm_offset_t, int);
+void		 pmap_qremove_quick (vm_offset_t, int);
+void		 pmap_qremove_noinval (vm_offset_t, int);
 void		 pmap_kenter (vm_offset_t, vm_paddr_t);
 int		 pmap_kenter_quick (vm_offset_t, vm_paddr_t);
 int		 pmap_kenter_noinval (vm_offset_t, vm_paddr_t);
-- 
2.7.2

