From b976e2c27a272fede34b02d1900430d3d5ccd624 Mon Sep 17 00:00:00 2001
From: zrj <rimvydas.jasinskas@gmail.com>
Date: Tue, 19 Jul 2016 10:07:45 +0300
Subject: [PATCH 054/100] sys: Extract CPUMASK macros to new
 <machine/cpumask.h>

There are plenty enough CPUMASK macros already for them to have their own header.
So far only userspace users are powerd(8), usched(8) and kern_usched.c(VKERNEL64).
After recent change to expose kernel internal CPUMASK macros those got available
for userland codes even through <time.h> header. It is better to avoid that.
Also this reduces POSIX namespace pollution and keeps cpu/types.h header slim.

For now leave CPUMASK_ELEMENTS (not sure about ASSYM() macro handling the _ prefix)
and cpumask_t typedef (forward decl of struct cpumask would be better in prototypes).
---
 sbin/usched/usched.c             |   1 +
 sys/cpu/x86_64/include/cpumask.h | 242 +++++++++++++++++++++++++++++++++++++++
 sys/cpu/x86_64/include/param.h   |   2 +-
 sys/cpu/x86_64/include/types.h   | 197 -------------------------------
 sys/kern/kern_usched.c           |   2 +-
 sys/platform/pc64/apic/lapic.h   |   4 +
 sys/sys/cpu_topology.h           |   2 +
 sys/sys/thread2.h                |   1 +
 usr.sbin/powerd/powerd.c         |   1 +
 9 files changed, 253 insertions(+), 199 deletions(-)
 create mode 100644 sys/cpu/x86_64/include/cpumask.h

diff --git a/sbin/usched/usched.c b/sbin/usched/usched.c
index e030a8f..213e921 100644
--- a/sbin/usched/usched.c
+++ b/sbin/usched/usched.c
@@ -35,6 +35,7 @@
 
 #include <sys/types.h>
 #include <sys/usched.h>
+#include <machine/cpumask.h>
 #include <stdio.h>
 #include <stdlib.h>
 #include <unistd.h>
diff --git a/sys/cpu/x86_64/include/cpumask.h b/sys/cpu/x86_64/include/cpumask.h
new file mode 100644
index 0000000..782031a
--- /dev/null
+++ b/sys/cpu/x86_64/include/cpumask.h
@@ -0,0 +1,242 @@
+/*
+ * Copyright (c) 2008-2016 The DragonFly Project.  All rights reserved.
+ *
+ * This code is derived from software contributed to The DragonFly Project
+ * by Matthew Dillon <dillon@backplane.com>
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ * 3. Neither the name of The DragonFly Project nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific, prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+ * FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE
+ * COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY OR CONSEQUENTIAL DAMAGES (INCLUDING,
+ * BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED
+ * AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+ * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
+ * OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ */
+
+#ifndef _CPU_CPUMASK_H_
+#define	_CPU_CPUMASK_H_
+
+#include <cpu/types.h>
+#include <cpu/atomic.h>
+
+#if CPUMASK_ELEMENTS != 4
+#error "CPUMASK macros incompatible with cpumask_t"
+#endif
+
+#define CPUMASK_INITIALIZER_ALLONES	{ .ary = { (__uint64_t)-1, \
+					  (__uint64_t)-1, \
+					  (__uint64_t)-1, \
+					  (__uint64_t)-1 } }
+#define CPUMASK_INITIALIZER_ONLYONE	{ .ary = { 1, 0, 0, 0 } }
+
+#define CPUMASK_SIMPLE(cpu)	((__uint64_t)1 << (cpu))
+
+#define CPUMASK_ADDR(mask, cpu)						\
+		(((cpu) < 64) ? &(mask).ary[0] :			\
+		(((cpu) < 128) ? &(mask).ary[1] :			\
+		(((cpu) < 192) ? &(mask).ary[2] : &(mask).ary[3])))
+
+#define BSRCPUMASK(val)		((val).ary[3] ? 192 + bsrq((val).ary[3]) : \
+				((val).ary[2] ? 128 + bsrq((val).ary[2]) : \
+				((val).ary[1] ? 64 + bsrq((val).ary[1]) : \
+						bsrq((val).ary[0]))))
+
+#define BSFCPUMASK(val)		((val).ary[0] ? bsfq((val).ary[0]) : \
+				((val).ary[1] ? 64 + bsfq((val).ary[1]) : \
+				((val).ary[2] ? 128 + bsfq((val).ary[2]) : \
+						192 + bsfq((val).ary[3]))))
+
+#define CPUMASK_CMPMASKEQ(val1, val2)	((val1).ary[0] == (val2).ary[0] && \
+					 (val1).ary[1] == (val2).ary[1] && \
+					 (val1).ary[2] == (val2).ary[2] && \
+					 (val1).ary[3] == (val2).ary[3])
+
+#define CPUMASK_CMPMASKNEQ(val1, val2)	((val1).ary[0] != (val2).ary[0] || \
+					 (val1).ary[1] != (val2).ary[1] || \
+					 (val1).ary[2] != (val2).ary[2] || \
+					 (val1).ary[3] != (val2).ary[3])
+
+#define CPUMASK_ISUP(val)		((val).ary[0] == 1 && \
+					 (val).ary[1] == 0 && \
+					 (val).ary[2] == 0 && \
+					 (val).ary[3] == 0)
+
+#define CPUMASK_TESTZERO(val)		((val).ary[0] == 0 && \
+					 (val).ary[1] == 0 && \
+					 (val).ary[2] == 0 && \
+					 (val).ary[3] == 0)
+
+#define CPUMASK_TESTNZERO(val)		((val).ary[0] != 0 || \
+					 (val).ary[1] != 0 || \
+					 (val).ary[2] != 0 || \
+					 (val).ary[3] != 0)
+
+#define CPUMASK_TESTBIT(val, i)		((val).ary[((i) >> 6) & 3] & \
+					 CPUMASK_SIMPLE((i) & 63))
+
+#define CPUMASK_TESTMASK(val1, val2)	(((val1).ary[0] & (val2.ary[0])) || \
+					 ((val1).ary[1] & (val2.ary[1])) || \
+					 ((val1).ary[2] & (val2.ary[2])) || \
+					 ((val1).ary[3] & (val2.ary[3])))
+
+#define CPUMASK_LOWMASK(val)		((val).ary[0])
+
+#define CPUMASK_ORBIT(mask, i)		((mask).ary[((i) >> 6) & 3] |= \
+					 CPUMASK_SIMPLE((i) & 63))
+
+#define CPUMASK_ANDBIT(mask, i)		((mask).ary[((i) >> 6) & 3] &= \
+					 CPUMASK_SIMPLE((i) & 63))
+
+#define CPUMASK_NANDBIT(mask, i)	((mask).ary[((i) >> 6) & 3] &= \
+					 ~CPUMASK_SIMPLE((i) & 63))
+
+#define CPUMASK_ASSZERO(mask)		do {				\
+					(mask).ary[0] = 0;		\
+					(mask).ary[1] = 0;		\
+					(mask).ary[2] = 0;		\
+					(mask).ary[3] = 0;		\
+					} while(0)
+
+#define CPUMASK_ASSALLONES(mask)	do {				\
+					(mask).ary[0] = (__uint64_t)-1;	\
+					(mask).ary[1] = (__uint64_t)-1;	\
+					(mask).ary[2] = (__uint64_t)-1;	\
+					(mask).ary[3] = (__uint64_t)-1;	\
+					} while(0)
+
+#define CPUMASK_ASSBIT(mask, i)		do {				\
+						CPUMASK_ASSZERO(mask);	\
+						CPUMASK_ORBIT(mask, i); \
+					} while(0)
+
+#define CPUMASK_ASSBMASK(mask, i)	do {				\
+		if (i < 64) {						\
+			(mask).ary[0] = CPUMASK_SIMPLE(i) - 1;		\
+			(mask).ary[1] = 0;				\
+			(mask).ary[2] = 0;				\
+			(mask).ary[3] = 0;				\
+		} else if (i < 128) {					\
+			(mask).ary[0] = (__uint64_t)-1;			\
+			(mask).ary[1] = CPUMASK_SIMPLE((i) - 64) - 1;	\
+			(mask).ary[2] = 0;				\
+			(mask).ary[3] = 0;				\
+		} else if (i < 192) {					\
+			(mask).ary[0] = (__uint64_t)-1;			\
+			(mask).ary[1] = (__uint64_t)-1;			\
+			(mask).ary[2] = CPUMASK_SIMPLE((i) - 128) - 1;	\
+			(mask).ary[3] = 0;				\
+		} else {						\
+			(mask).ary[0] = (__uint64_t)-1;			\
+			(mask).ary[1] = (__uint64_t)-1;			\
+			(mask).ary[2] = (__uint64_t)-1;			\
+			(mask).ary[3] = CPUMASK_SIMPLE((i) - 192) - 1;	\
+		}							\
+					} while(0)
+
+#define CPUMASK_ASSNBMASK(mask, i)	do {				\
+		if (i < 64) {						\
+			(mask).ary[0] = ~(CPUMASK_SIMPLE(i) - 1);	\
+			(mask).ary[1] = (__uint64_t)-1;			\
+			(mask).ary[2] = (__uint64_t)-1;			\
+			(mask).ary[3] = (__uint64_t)-1;			\
+		} else if (i < 128) {					\
+			(mask).ary[0] = 0;				\
+			(mask).ary[1] = ~(CPUMASK_SIMPLE((i) - 64) - 1);\
+			(mask).ary[2] = (__uint64_t)-1;			\
+			(mask).ary[3] = (__uint64_t)-1;			\
+		} else if (i < 192) {					\
+			(mask).ary[0] = 0;				\
+			(mask).ary[1] = 0;				\
+			(mask).ary[2] = ~(CPUMASK_SIMPLE((i) - 128) - 1);\
+			(mask).ary[3] = (__uint64_t)-1;			\
+		} else {						\
+			(mask).ary[0] = 0;				\
+			(mask).ary[1] = 0;				\
+			(mask).ary[2] = 0;				\
+			(mask).ary[3] = ~(CPUMASK_SIMPLE((i) - 192) - 1);\
+		}							\
+					} while(0)
+
+#define CPUMASK_ANDMASK(mask, val)	do {				\
+					(mask).ary[0] &= (val).ary[0];	\
+					(mask).ary[1] &= (val).ary[1];	\
+					(mask).ary[2] &= (val).ary[2];	\
+					(mask).ary[3] &= (val).ary[3];	\
+					} while(0)
+
+#define CPUMASK_NANDMASK(mask, val)	do {				\
+					(mask).ary[0] &= ~(val).ary[0];	\
+					(mask).ary[1] &= ~(val).ary[1];	\
+					(mask).ary[2] &= ~(val).ary[2];	\
+					(mask).ary[3] &= ~(val).ary[3];	\
+					} while(0)
+
+#define CPUMASK_ORMASK(mask, val)	do {				\
+					(mask).ary[0] |= (val).ary[0];	\
+					(mask).ary[1] |= (val).ary[1];	\
+					(mask).ary[2] |= (val).ary[2];	\
+					(mask).ary[3] |= (val).ary[3];	\
+					} while(0)
+
+#define CPUMASK_XORMASK(mask, val)	do {				\
+					(mask).ary[0] ^= (val).ary[0];	\
+					(mask).ary[1] ^= (val).ary[1];	\
+					(mask).ary[2] ^= (val).ary[2];	\
+					(mask).ary[3] ^= (val).ary[3];	\
+					} while(0)
+
+#define ATOMIC_CPUMASK_ORBIT(mask, i)					  \
+			atomic_set_cpumask(&(mask).ary[((i) >> 6) & 3],	  \
+					   CPUMASK_SIMPLE((i) & 63))
+
+#define ATOMIC_CPUMASK_NANDBIT(mask, i)					  \
+			atomic_clear_cpumask(&(mask).ary[((i) >> 6) & 3], \
+					   CPUMASK_SIMPLE((i) & 63))
+
+#define ATOMIC_CPUMASK_TESTANDSET(mask, i)				  \
+		atomic_testandset_long(&(mask).ary[((i) >> 6) & 3], (i))
+
+#define ATOMIC_CPUMASK_TESTANDCLR(mask, i)				  \
+		atomic_testandclear_long(&(mask).ary[((i) >> 6) & 3], (i))
+
+#define ATOMIC_CPUMASK_ORMASK(mask, val) do {				  \
+			atomic_set_cpumask(&(mask).ary[0], (val).ary[0]); \
+			atomic_set_cpumask(&(mask).ary[1], (val).ary[1]); \
+			atomic_set_cpumask(&(mask).ary[2], (val).ary[2]); \
+			atomic_set_cpumask(&(mask).ary[3], (val).ary[3]); \
+					 } while(0)
+
+#define ATOMIC_CPUMASK_NANDMASK(mask, val) do {				    \
+			atomic_clear_cpumask(&(mask).ary[0], (val).ary[0]); \
+			atomic_clear_cpumask(&(mask).ary[1], (val).ary[1]); \
+			atomic_clear_cpumask(&(mask).ary[2], (val).ary[2]); \
+			atomic_clear_cpumask(&(mask).ary[3], (val).ary[3]); \
+					 } while(0)
+
+#define ATOMIC_CPUMASK_COPY(mask, val) do {				    \
+			atomic_store_rel_cpumask(&(mask).ary[0], (val).ary[0]);\
+			atomic_store_rel_cpumask(&(mask).ary[1], (val).ary[1]);\
+			atomic_store_rel_cpumask(&(mask).ary[2], (val).ary[2]);\
+			atomic_store_rel_cpumask(&(mask).ary[3], (val).ary[3]);\
+					 } while(0)
+
+#endif /* !_CPU_CPUMASK_H_ */
diff --git a/sys/cpu/x86_64/include/param.h b/sys/cpu/x86_64/include/param.h
index 9dd16b8..26f5587 100644
--- a/sys/cpu/x86_64/include/param.h
+++ b/sys/cpu/x86_64/include/param.h
@@ -71,7 +71,7 @@
  * Use SMP_MAXCPU instead of MAXCPU for structures that are intended to
  * remain compatible between UP and SMP builds.
  *
- * WARNING!  CPUMASK macros in include/types.h must also be adjusted,
+ * WARNING!  CPUMASK macros in include/cpumask.h must also be adjusted,
  *	     as well as any assembly.  Be sure that CPUMASK_ELEMENTS
  *	     is always correct so incompatible assembly #error's out
  *	     during the kernel compile.
diff --git a/sys/cpu/x86_64/include/types.h b/sys/cpu/x86_64/include/types.h
index 41a595a..b1b459b 100644
--- a/sys/cpu/x86_64/include/types.h
+++ b/sys/cpu/x86_64/include/types.h
@@ -83,203 +83,6 @@ typedef struct {
 	__uint64_t      ary[4];
 } cpumask_t;
 
-#define CPUMASK_INITIALIZER_ALLONES	{ .ary = { (__uint64_t)-1, \
-					  (__uint64_t)-1, \
-					  (__uint64_t)-1, \
-					  (__uint64_t)-1 } }
-#define CPUMASK_INITIALIZER_ONLYONE	{ .ary = { 1, 0, 0, 0 } }
-
-#define CPUMASK_SIMPLE(cpu)	((__uint64_t)1 << (cpu))
-
-#define CPUMASK_ADDR(mask, cpu)						\
-		(((cpu) < 64) ? &(mask).ary[0] :			\
-		(((cpu) < 128) ? &(mask).ary[1] :			\
-		(((cpu) < 192) ? &(mask).ary[2] : &(mask).ary[3])))
-
-#define BSRCPUMASK(val)		((val).ary[3] ? 192 + bsrq((val).ary[3]) : \
-				((val).ary[2] ? 128 + bsrq((val).ary[2]) : \
-				((val).ary[1] ? 64 + bsrq((val).ary[1]) : \
-						bsrq((val).ary[0]))))
-
-#define BSFCPUMASK(val)		((val).ary[0] ? bsfq((val).ary[0]) : \
-				((val).ary[1] ? 64 + bsfq((val).ary[1]) : \
-				((val).ary[2] ? 128 + bsfq((val).ary[2]) : \
-						192 + bsfq((val).ary[3]))))
-
-#define CPUMASK_CMPMASKEQ(val1, val2)	((val1).ary[0] == (val2).ary[0] && \
-					 (val1).ary[1] == (val2).ary[1] && \
-					 (val1).ary[2] == (val2).ary[2] && \
-					 (val1).ary[3] == (val2).ary[3])
-
-#define CPUMASK_CMPMASKNEQ(val1, val2)	((val1).ary[0] != (val2).ary[0] || \
-					 (val1).ary[1] != (val2).ary[1] || \
-					 (val1).ary[2] != (val2).ary[2] || \
-					 (val1).ary[3] != (val2).ary[3])
-
-#define CPUMASK_ISUP(val)		((val).ary[0] == 1 && \
-					 (val).ary[1] == 0 && \
-					 (val).ary[2] == 0 && \
-					 (val).ary[3] == 0)
-
-#define CPUMASK_TESTZERO(val)		((val).ary[0] == 0 && \
-					 (val).ary[1] == 0 && \
-					 (val).ary[2] == 0 && \
-					 (val).ary[3] == 0)
-
-#define CPUMASK_TESTNZERO(val)		((val).ary[0] != 0 || \
-					 (val).ary[1] != 0 || \
-					 (val).ary[2] != 0 || \
-					 (val).ary[3] != 0)
-
-#define CPUMASK_TESTBIT(val, i)		((val).ary[((i) >> 6) & 3] & \
-					 CPUMASK_SIMPLE((i) & 63))
-
-#define CPUMASK_TESTMASK(val1, val2)	(((val1).ary[0] & (val2.ary[0])) || \
-					 ((val1).ary[1] & (val2.ary[1])) || \
-					 ((val1).ary[2] & (val2.ary[2])) || \
-					 ((val1).ary[3] & (val2.ary[3])))
-
-#define CPUMASK_LOWMASK(val)		((val).ary[0])
-
-#define CPUMASK_ORBIT(mask, i)		((mask).ary[((i) >> 6) & 3] |= \
-					 CPUMASK_SIMPLE((i) & 63))
-
-#define CPUMASK_ANDBIT(mask, i)		((mask).ary[((i) >> 6) & 3] &= \
-					 CPUMASK_SIMPLE((i) & 63))
-
-#define CPUMASK_NANDBIT(mask, i)	((mask).ary[((i) >> 6) & 3] &= \
-					 ~CPUMASK_SIMPLE((i) & 63))
-
-#define CPUMASK_ASSZERO(mask)		do {				\
-					(mask).ary[0] = 0;		\
-					(mask).ary[1] = 0;		\
-					(mask).ary[2] = 0;		\
-					(mask).ary[3] = 0;		\
-					} while(0)
-
-#define CPUMASK_ASSALLONES(mask)	do {				\
-					(mask).ary[0] = (__uint64_t)-1;	\
-					(mask).ary[1] = (__uint64_t)-1;	\
-					(mask).ary[2] = (__uint64_t)-1;	\
-					(mask).ary[3] = (__uint64_t)-1;	\
-					} while(0)
-
-#define CPUMASK_ASSBIT(mask, i)		do {				\
-						CPUMASK_ASSZERO(mask);	\
-						CPUMASK_ORBIT(mask, i); \
-					} while(0)
-
-#define CPUMASK_ASSBMASK(mask, i)	do {				\
-		if (i < 64) {						\
-			(mask).ary[0] = CPUMASK_SIMPLE(i) - 1;		\
-			(mask).ary[1] = 0;				\
-			(mask).ary[2] = 0;				\
-			(mask).ary[3] = 0;				\
-		} else if (i < 128) {					\
-			(mask).ary[0] = (__uint64_t)-1;			\
-			(mask).ary[1] = CPUMASK_SIMPLE((i) - 64) - 1;	\
-			(mask).ary[2] = 0;				\
-			(mask).ary[3] = 0;				\
-		} else if (i < 192) {					\
-			(mask).ary[0] = (__uint64_t)-1;			\
-			(mask).ary[1] = (__uint64_t)-1;			\
-			(mask).ary[2] = CPUMASK_SIMPLE((i) - 128) - 1;	\
-			(mask).ary[3] = 0;				\
-		} else {						\
-			(mask).ary[0] = (__uint64_t)-1;			\
-			(mask).ary[1] = (__uint64_t)-1;			\
-			(mask).ary[2] = (__uint64_t)-1;			\
-			(mask).ary[3] = CPUMASK_SIMPLE((i) - 192) - 1;	\
-		}							\
-					} while(0)
-
-#define CPUMASK_ASSNBMASK(mask, i)	do {				\
-		if (i < 64) {						\
-			(mask).ary[0] = ~(CPUMASK_SIMPLE(i) - 1);	\
-			(mask).ary[1] = (__uint64_t)-1;			\
-			(mask).ary[2] = (__uint64_t)-1;			\
-			(mask).ary[3] = (__uint64_t)-1;			\
-		} else if (i < 128) {					\
-			(mask).ary[0] = 0;				\
-			(mask).ary[1] = ~(CPUMASK_SIMPLE((i) - 64) - 1);\
-			(mask).ary[2] = (__uint64_t)-1;			\
-			(mask).ary[3] = (__uint64_t)-1;			\
-		} else if (i < 192) {					\
-			(mask).ary[0] = 0;				\
-			(mask).ary[1] = 0;				\
-			(mask).ary[2] = ~(CPUMASK_SIMPLE((i) - 128) - 1);\
-			(mask).ary[3] = (__uint64_t)-1;			\
-		} else {						\
-			(mask).ary[0] = 0;				\
-			(mask).ary[1] = 0;				\
-			(mask).ary[2] = 0;				\
-			(mask).ary[3] = ~(CPUMASK_SIMPLE((i) - 192) - 1);\
-		}							\
-					} while(0)
-
-#define CPUMASK_ANDMASK(mask, val)	do {				\
-					(mask).ary[0] &= (val).ary[0];	\
-					(mask).ary[1] &= (val).ary[1];	\
-					(mask).ary[2] &= (val).ary[2];	\
-					(mask).ary[3] &= (val).ary[3];	\
-					} while(0)
-
-#define CPUMASK_NANDMASK(mask, val)	do {				\
-					(mask).ary[0] &= ~(val).ary[0];	\
-					(mask).ary[1] &= ~(val).ary[1];	\
-					(mask).ary[2] &= ~(val).ary[2];	\
-					(mask).ary[3] &= ~(val).ary[3];	\
-					} while(0)
-
-#define CPUMASK_ORMASK(mask, val)	do {				\
-					(mask).ary[0] |= (val).ary[0];	\
-					(mask).ary[1] |= (val).ary[1];	\
-					(mask).ary[2] |= (val).ary[2];	\
-					(mask).ary[3] |= (val).ary[3];	\
-					} while(0)
-
-#define CPUMASK_XORMASK(mask, val)	do {				\
-					(mask).ary[0] ^= (val).ary[0];	\
-					(mask).ary[1] ^= (val).ary[1];	\
-					(mask).ary[2] ^= (val).ary[2];	\
-					(mask).ary[3] ^= (val).ary[3];	\
-					} while(0)
-
-#define ATOMIC_CPUMASK_ORBIT(mask, i)					  \
-			atomic_set_cpumask(&(mask).ary[((i) >> 6) & 3],	  \
-					   CPUMASK_SIMPLE((i) & 63))
-
-#define ATOMIC_CPUMASK_NANDBIT(mask, i)					  \
-			atomic_clear_cpumask(&(mask).ary[((i) >> 6) & 3], \
-					   CPUMASK_SIMPLE((i) & 63))
-
-#define ATOMIC_CPUMASK_TESTANDSET(mask, i)				  \
-		atomic_testandset_long(&(mask).ary[((i) >> 6) & 3], (i))
-
-#define ATOMIC_CPUMASK_TESTANDCLR(mask, i)				  \
-		atomic_testandclear_long(&(mask).ary[((i) >> 6) & 3], (i))
-
-#define ATOMIC_CPUMASK_ORMASK(mask, val) do {				  \
-			atomic_set_cpumask(&(mask).ary[0], (val).ary[0]); \
-			atomic_set_cpumask(&(mask).ary[1], (val).ary[1]); \
-			atomic_set_cpumask(&(mask).ary[2], (val).ary[2]); \
-			atomic_set_cpumask(&(mask).ary[3], (val).ary[3]); \
-					 } while(0)
-
-#define ATOMIC_CPUMASK_NANDMASK(mask, val) do {				    \
-			atomic_clear_cpumask(&(mask).ary[0], (val).ary[0]); \
-			atomic_clear_cpumask(&(mask).ary[1], (val).ary[1]); \
-			atomic_clear_cpumask(&(mask).ary[2], (val).ary[2]); \
-			atomic_clear_cpumask(&(mask).ary[3], (val).ary[3]); \
-					 } while(0)
-
-#define ATOMIC_CPUMASK_COPY(mask, val) do {				    \
-			atomic_store_rel_cpumask(&(mask).ary[0], (val).ary[0]);\
-			atomic_store_rel_cpumask(&(mask).ary[1], (val).ary[1]);\
-			atomic_store_rel_cpumask(&(mask).ary[2], (val).ary[2]);\
-			atomic_store_rel_cpumask(&(mask).ary[3], (val).ary[3]);\
-					 } while(0)
-
 #define CPULOCK_EXCLBIT	0		/* exclusive lock bit number */
 #define CPULOCK_EXCL	0x00000001	/* exclusive lock */
 #define CPULOCK_INCR	0x00000002	/* auxillary counter add/sub */
diff --git a/sys/kern/kern_usched.c b/sys/kern/kern_usched.c
index 9b81419..37973a4 100644
--- a/sys/kern/kern_usched.c
+++ b/sys/kern/kern_usched.c
@@ -31,7 +31,6 @@
  * OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  * 
- * $DragonFly: src/sys/kern/kern_usched.c,v 1.9 2007/07/02 17:06:55 dillon Exp $
  */
 
 #include <sys/errno.h>
@@ -44,6 +43,7 @@
 
 #include <sys/mplock2.h>
 
+#include <machine/cpumask.h>
 #include <machine/smp.h>
 
 static TAILQ_HEAD(, usched) usched_list = TAILQ_HEAD_INITIALIZER(usched_list);
diff --git a/sys/platform/pc64/apic/lapic.h b/sys/platform/pc64/apic/lapic.h
index bf607c0..0c61808 100644
--- a/sys/platform/pc64/apic/lapic.h
+++ b/sys/platform/pc64/apic/lapic.h
@@ -82,6 +82,10 @@ int	single_apic_ipi_passive(int, int, int);
 /*
  * Send an IPI INTerrupt containing 'vector' to all CPUs EXCEPT myself
  */
+#ifndef _CPU_CPUMASK_H_
+#include <machine/cpumask.h>
+#endif
+
 static __inline int
 all_but_self_ipi(int vector)
 {
diff --git a/sys/sys/cpu_topology.h b/sys/sys/cpu_topology.h
index 5cd5b65..5731450 100644
--- a/sys/sys/cpu_topology.h
+++ b/sys/sys/cpu_topology.h
@@ -3,6 +3,8 @@
 
 #ifdef _KERNEL
 
+#include <machine/cpumask.h>
+
 /* CPU TOPOLOGY DATA AND FUNCTIONS */
 struct cpu_node {
 	struct cpu_node * parent_node;
diff --git a/sys/sys/thread2.h b/sys/sys/thread2.h
index 0fce984..ccd93a0 100644
--- a/sys/sys/thread2.h
+++ b/sys/sys/thread2.h
@@ -28,6 +28,7 @@
 #include <sys/globaldata.h>
 #endif
 #include <machine/cpufunc.h>
+#include <machine/cpumask.h>
 
 /*
  * Is a token held either by the specified thread or held shared?
diff --git a/usr.sbin/powerd/powerd.c b/usr.sbin/powerd/powerd.c
index 614ce5e..f2c8adb 100644
--- a/usr.sbin/powerd/powerd.c
+++ b/usr.sbin/powerd/powerd.c
@@ -49,6 +49,7 @@
 #include <sys/soundcard.h>
 #include <sys/time.h>
 #include <machine/cpufunc.h>
+#include <machine/cpumask.h>
 #include <err.h>
 #include <stdio.h>
 #include <stdlib.h>
-- 
2.7.2

